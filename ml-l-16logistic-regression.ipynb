{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression\n\nLogistic regression aims to solve classification problems. It does this by predicting categorical outcomes, unlike linear regression that predicts a continuous outcome.\n\nIn the simplest case there are two outcomes, which is called binomial, an example of which is predicting if a tumor is malignant or benign. Other cases have more than two outcomes to classify, in this case it is called multinomial. A common example for multinomial logistic regression would be predicting the class of an iris flower between 3 different species.\n\nHere we will be using basic logistic regression to predict a binomial variable. This means it has only two possible outcomes.","metadata":{}},{"cell_type":"markdown","source":"# How does it work?\n\nIn Python we have modules that will do the work for us. Start by importing the NumPy module.","metadata":{}},{"cell_type":"code","source":"import numpy","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.219179Z","iopub.execute_input":"2022-10-01T05:36:37.219609Z","iopub.status.idle":"2022-10-01T05:36:37.223888Z","shell.execute_reply.started":"2022-10-01T05:36:37.219578Z","shell.execute_reply":"2022-10-01T05:36:37.222894Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Store the independent variables in X.\n\nStore the dependent variable in y.\n\nBelow is a sample dataset:","metadata":{}},{"cell_type":"code","source":"#X represents the size of a tumor in centimeters.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n\n#Note: X has to be reshaped into a column from a row for the LogisticRegression() function to work.\n#y represents whether or not the tumor is cancerous (0 for \"No\", 1 for \"Yes\").\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.229181Z","iopub.execute_input":"2022-10-01T05:36:37.229766Z","iopub.status.idle":"2022-10-01T05:36:37.236896Z","shell.execute_reply.started":"2022-10-01T05:36:37.229733Z","shell.execute_reply":"2022-10-01T05:36:37.235953Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We will use a method from the sklearn module, so we will have to import that module as well:","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.238858Z","iopub.execute_input":"2022-10-01T05:36:37.239269Z","iopub.status.idle":"2022-10-01T05:36:37.253668Z","shell.execute_reply.started":"2022-10-01T05:36:37.239237Z","shell.execute_reply":"2022-10-01T05:36:37.252657Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"From the sklearn module we will use the LogisticRegression() method to create a logistic regression object.\n\nThis object has a method called fit() that takes the independent and dependent values as parameters and fills the regression object with data that describes the relationship:","metadata":{}},{"cell_type":"code","source":"logr = linear_model.LogisticRegression()\nlogr.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.254858Z","iopub.execute_input":"2022-10-01T05:36:37.255485Z","iopub.status.idle":"2022-10-01T05:36:37.273413Z","shell.execute_reply.started":"2022-10-01T05:36:37.255450Z","shell.execute_reply":"2022-10-01T05:36:37.272360Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we have a logistic regression object that is ready to whether a tumor is cancerous based on the tumor size:","metadata":{}},{"cell_type":"code","source":"#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.275684Z","iopub.execute_input":"2022-10-01T05:36:37.276006Z","iopub.status.idle":"2022-10-01T05:36:37.281726Z","shell.execute_reply.started":"2022-10-01T05:36:37.275979Z","shell.execute_reply":"2022-10-01T05:36:37.280826Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Example\n\nSee the whole example in action:","metadata":{}},{"cell_type":"code","source":"import numpy\nfrom sklearn import linear_model\n\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n\n#predict if tumor is cancerous where the size is 3.46mm:\npredicted = logr.predict(numpy.array([3.46]).reshape(-1,1))\nprint(predicted) ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.282946Z","iopub.execute_input":"2022-10-01T05:36:37.283272Z","iopub.status.idle":"2022-10-01T05:36:37.300631Z","shell.execute_reply.started":"2022-10-01T05:36:37.283242Z","shell.execute_reply":"2022-10-01T05:36:37.299814Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We have predicted that a tumor with a size of 3.46mm will not be cancerous.","metadata":{}},{"cell_type":"markdown","source":"# Coefficient\n\nIn logistic regression the coefficient is the expected change in log-odds of having the outcome per unit change in X.\n\nThis does not have the most intuitive understanding so let's use it to create something that makes more sense, odds.\n# Example\n\nSee the whole example in action:","metadata":{}},{"cell_type":"code","source":"import numpy\nfrom sklearn import linear_model\n\n#Reshaped for Logistic function.\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n\nlog_odds = logr.coef_\nodds = numpy.exp(log_odds)\n\nprint(odds) ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.301998Z","iopub.execute_input":"2022-10-01T05:36:37.302348Z","iopub.status.idle":"2022-10-01T05:36:37.314787Z","shell.execute_reply.started":"2022-10-01T05:36:37.302317Z","shell.execute_reply":"2022-10-01T05:36:37.314044Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[4.03541657]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This tells us that as the size of a tumor increases by 1mm the odds of it being a tumor increases by 4x.","metadata":{}},{"cell_type":"markdown","source":"# Probability\n\nThe coefficient and intercept values can be used to find the probability that each tumor is cancerous.\n\nCreate a function that uses the model's coefficient and intercept values to return a new value. This new value represents probability that the given observation is a tumor:","metadata":{}},{"cell_type":"code","source":"def logit2prob(logr,x):\n  log_odds = logr.coef_ * x + logr.intercept_\n  odds = numpy.exp(log_odds)\n  probability = odds / (1 + odds)\n  return(probability)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.356709Z","iopub.execute_input":"2022-10-01T05:36:37.357788Z","iopub.status.idle":"2022-10-01T05:36:37.362714Z","shell.execute_reply.started":"2022-10-01T05:36:37.357751Z","shell.execute_reply":"2022-10-01T05:36:37.361795Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Function Explained\n\nTo find the log-odds for each observation, we must first create a formula that looks similar to the one from linear regression, extracting the coefficient and the intercept.","metadata":{}},{"cell_type":"code","source":"log_odds = logr.coef_ * x + logr.intercept_ ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:36:37.364505Z","iopub.execute_input":"2022-10-01T05:36:37.364811Z","iopub.status.idle":"2022-10-01T05:36:37.388060Z","shell.execute_reply.started":"2022-10-01T05:36:37.364783Z","shell.execute_reply":"2022-10-01T05:36:37.386749Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/1912545679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_odds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"],"ename":"NameError","evalue":"name 'x' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"To then convert the log-odds to odds we must exponentiate the log-odds.","metadata":{}},{"cell_type":"code","source":"odds = numpy.exp(log_odds)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:38:09.347821Z","iopub.execute_input":"2022-10-01T05:38:09.348258Z","iopub.status.idle":"2022-10-01T05:38:09.353943Z","shell.execute_reply.started":"2022-10-01T05:38:09.348221Z","shell.execute_reply":"2022-10-01T05:38:09.352543Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the odds, we can convert it to probability by dividing it by 1 plus the odds.","metadata":{}},{"cell_type":"code","source":"probability = odds / (1 + odds)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:39:12.884285Z","iopub.execute_input":"2022-10-01T05:39:12.885083Z","iopub.status.idle":"2022-10-01T05:39:12.889920Z","shell.execute_reply.started":"2022-10-01T05:39:12.885038Z","shell.execute_reply":"2022-10-01T05:39:12.889069Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Let us now use the function with what we have learned to find out the probability that each tumor is cancerous.\n# Example\n\nSee the whole example in action:","metadata":{}},{"cell_type":"code","source":"import numpy\nfrom sklearn import linear_model\n\nX = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\ny = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n\nlogr = linear_model.LogisticRegression()\nlogr.fit(X,y)\n\ndef logit2prob(logr, X):\n  log_odds = logr.coef_ * X + logr.intercept_\n  odds = numpy.exp(log_odds)\n  probability = odds / (1 + odds)\n  return(probability)\n\nprint(logit2prob(logr, X)) ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T05:40:20.228967Z","iopub.execute_input":"2022-10-01T05:40:20.229348Z","iopub.status.idle":"2022-10-01T05:40:20.243968Z","shell.execute_reply.started":"2022-10-01T05:40:20.229318Z","shell.execute_reply":"2022-10-01T05:40:20.242844Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[[0.60749955]\n [0.19268876]\n [0.12775886]\n [0.00955221]\n [0.08038616]\n [0.07345637]\n [0.88362743]\n [0.77901378]\n [0.88924409]\n [0.81293497]\n [0.57719129]\n [0.96664243]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Results Explained\n\n3.78 0.61 The probability that a tumor with the size 3.78cm is cancerous is 61%.\n\n2.44 0.19 The probability that a tumor with the size 2.44cm is cancerous is 19%.\n\n2.09 0.13 The probability that a tumor with the size 2.09cm is cancerous is 13%.","metadata":{}}]}